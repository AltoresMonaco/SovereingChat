services:
  api:
    build:
      context: .
      dockerfile: Dockerfile
    image: sovereignchat:latest
    ports:
      - "3080:3080"
    # user: "1000:1000"  # Commenté pour éviter les problèmes de permissions
    environment:
      - HOST=0.0.0.0
      - PORT=3080
      - MONGO_URI=${MONGO_URI}
      - MEILI_HOST=${MEILI_HOST}
      - MEILI_MASTER_KEY=${MEILI_MASTER_KEY}
      - MEILI_NO_ANALYTICS=${MEILI_NO_ANALYTICS}
      - RAG_ENABLED=${RAG_ENABLED}
      - RAG_API_URL=${RAG_API_URL}
      - RAG_PORT=${RAG_PORT:-8000}
      - CREDS_KEY=${CREDS_KEY}
      - CREDS_IV=${CREDS_IV}
      - SEARCH=${SEARCH}
      - ALLOW_REGISTRATION=${ALLOW_REGISTRATION}
      - ALLOW_EMAIL_LOGIN=${ALLOW_EMAIL_LOGIN}
      - ALLOW_SOCIAL_LOGIN=${ALLOW_SOCIAL_LOGIN}
      - JWT_SECRET=${JWT_SECRET}
      - CUSTOM_FOOTER=[Chat Souverain](/) made by [Altores](https://altores.app), powered by [![Monaco Cloud](/assets/MonacoCloudLogo.png)](https://www.monacocloud.mc/)
    volumes:
      - /opt/apps/sovereignchat/librechat.yaml:/app/librechat.yaml:ro
      - /srv/librechat-data/uploads:/app/uploads
      - /srv/librechat-data/uploads:/app/client/public/uploads
      - /srv/librechat-data/images:/app/client/public/images
      - /srv/librechat-data/librechat-logs:/app/api/logs
    networks:
      - default

  mongodb:
    image: alpine:3.19
    command: ["sh","-c","tail -f /dev/null"]
    restart: always

  rag_api:
    image: alpine:3.19
    command: ["sh","-c","tail -f /dev/null"]
    restart: always

  vectordb:
    image: alpine:3.19
    command: ["sh","-c","tail -f /dev/null"]
    restart: always

  meilisearch: { profiles: ["disabled"] }

  ollama:
    image: ollama/ollama:latest
    restart: always
    environment:
      - OLLAMA_KEEP_ALIVE=5m  # Réduit de 24h à 5 minutes pour libérer la mémoire rapidement
      - OLLAMA_NUM_PARALLEL=1  # Limite à 1 modèle en parallèle
      - OLLAMA_MAX_LOADED_MODELS=1  # Maximum 1 modèle chargé à la fois
      - OLLAMA_FLASH_ATTENTION=1  # Active l'optimisation mémoire
    volumes:
      - /srv/librechat-data/ollama:/root/.ollama
    networks:
      - default
    deploy:
      resources:
        limits:
          memory: 8G  # Limite mémoire à 8GB max
        reservations:
          memory: 4G

  minicpm:
    image: openmmlab/lmdeploy:latest
    restart: always
    command: ["bash","-lc","MODEL_DIR=/models/minicpm-v-4_5; if [ -d \"$MODEL_DIR\" ]; then echo 'Starting LMDeploy with model at' $MODEL_DIR; lmdeploy serve api_server $MODEL_DIR --server-port 8000 --api-keys dummy --backend turbomind; else echo 'Model missing at' $MODEL_DIR '— place FP16 weights there to stay 0‑egress'; sleep infinity; fi"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - /srv/librechat-data/minicpm:/models:ro
    networks:
      - default

  llamacpp:
    image: ghcr.io/ggerganov/llama.cpp:server
    command: ["-m","/models/MiniCPM-V-4_5-Q8_0.gguf","--host","0.0.0.0","--port","8080","-c","8192","-ngl","0"]
    volumes:
      - /srv/librechat-data/ollama/models/minicpm:/models:ro
    networks:
      - default

networks:
  default:
    external: true
    name: coolify
  coolify:
    external: true